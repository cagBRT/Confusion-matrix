{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "confusionMatrix1.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyNyFK2c1vvBtxZNJ847OBdm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/Confusion-matrix/blob/master/confusionMatrix1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97MJzGRmDGl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/cagBRT/Confusion-matrix.git cloned-repo\n",
        "%cd cloned-repo\n",
        "!ls\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAh8c8TWDL3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image\n",
        "def page(num):\n",
        "    return Image(\"confusionMatrix\"+str(num)+ \".png\" , width=640)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwIYuS6jqeJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Python script for confusion matrix creation.\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1PE4Kuw-5wS",
        "colab_type": "text"
      },
      "source": [
        "**Example 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnEk2aY39c4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_true = [2, 0, 2, 2, 0, 1] #ground truth\n",
        "y_pred = [0, 0, 2, 2, 0, 2] #prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD9fr4KC-99U",
        "colab_type": "text"
      },
      "source": [
        "**The confusion matrix**<br>\n",
        "An array of shape (n_classes, n_classes)<br>\n",
        "Confusion matrix whose i-th row and j-th column entry indicates the number of <br>samples with true label being i-th class and prediced label being j-th class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC2A_6yj-4lZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "confusion_matrix(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLYcI18JDbSx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "page(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOyyezi9F6rb",
        "colab_type": "text"
      },
      "source": [
        "The total of the values in the confusion matrix equals the total number of predictions made."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohLU8qhsDmg0",
        "colab_type": "text"
      },
      "source": [
        "**Quiz 1**<br>\n",
        "confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])<br>\n",
        ">[[2,0,1]<br>\n",
        "[0,0,1]<br>\n",
        "[1,0,3]]<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URyfI4btEHfJ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title How many times was a cat mislabeled as an ant or bird?\n",
        "ant_wrong =  5#@param {type:\"integer\"}\n",
        "if(ant_wrong == 1):\n",
        "  print(\"Correct!\")\n",
        "else:\n",
        "  print(\"Try again\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1aeYxzOFQ5w",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title How many times was an ant mislabeled as an cat or bird?\n",
        "cat_wrong =  5#@param {type:\"integer\"}\n",
        "if(cat_wrong == 1):\n",
        "  print(\"Correct!\")\n",
        "else:\n",
        "  print(\"Try again\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0USUzkJPFnmb",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title How many times was a bird mislabeled as an ant or cat?\n",
        "bird_wrong =  5#@param {type:\"integer\"}\n",
        "if(bird_wrong == 1):\n",
        "  print(\"Correct!\")\n",
        "else:\n",
        "  print(\"Try again\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUKpvpncDod5",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title \n",
        "y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\",\"cat\",\"ant\"]\n",
        "y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\", \"cat\",\"cat\"]\n",
        "confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ_UGFxMEJRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "page(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3GsNhTTIK1W",
        "colab_type": "text"
      },
      "source": [
        "**Quiz 2**<br>\n",
        "The model is predicting whether the image is of a cat or not. \n",
        "\n",
        "Actual values =    [‘dog’, ‘cat’, ‘dog’, ‘cat’, ‘dog’, ‘dog’, ‘cat’, ‘dog’, ‘cat’, ‘dog’]<br>\n",
        "Predicted values = [‘dog’, ‘dog’, ‘dog’, ‘cat’, ‘dog’, ‘dog’, ‘cat’, ‘cat’, ‘cat’, ‘cat’]<br>\n",
        "<br>\n",
        "We describe predicted values as Positive/Negative and actual values as True/False.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg5eQ-jJaC86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cm = [[1,0,1,0,1,1,0,1,0,1], [1,1,1,0,1,1,0,0,0,0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEsWecbPEuMG",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title How many True Positives?\n",
        "tp = 0 #@param {type:\"integer\"}\n",
        "if tp==3:\n",
        "  print(\"Correct!\")\n",
        "else:\n",
        "  print(\"Try again\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79FcRflAFL7b",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title How many True Negatives?\n",
        "tp = 0 #@param {type:\"integer\"}\n",
        "if tp==4:\n",
        "  print(\"Correct!\")\n",
        "else:\n",
        "  print(\"Try again\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_zw7_6CFlyT",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title How many False Positives?\n",
        "tp = 0 #@param {type:\"integer\"}\n",
        "if tp==2:\n",
        "  print(\"Correct!\")\n",
        "else:\n",
        "  print(\"Try again\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3WuZyQuFqmA",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title How many False Negatives?\n",
        "tp = 0 #@param {type:\"integer\"}\n",
        "if tp==1:\n",
        "  print(\"Correct!\")\n",
        "else:\n",
        "  print(\"Try again\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcvYjuxHEsdL",
        "colab_type": "text"
      },
      "source": [
        "# **Binary Classification Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnxOgZP5Ex65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "page(8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnNfqGdUbyMC",
        "colab_type": "text"
      },
      "source": [
        "# **Accuracy:** <br>\n",
        "Gives an overall accuracy of the model, meaning the fraction of the total samples that were correctly classified by the classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C72BJ1vaE11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "page(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fl7XBtlrakRg",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title What is the accuracy for the Cat Identification Model? (In decimal format)\n",
        "accuracy =  0.8#@param {type:\"number\"}\n",
        "if accuracy == .7:\n",
        "  print(\"Correct!\")\n",
        "else:\n",
        "  print(\"Try again\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWizwiRpiats",
        "colab_type": "text"
      },
      "source": [
        "# **Recall (aka Sensitivity):**<br>\n",
        "\n",
        "Recall is defined as the ratio of the total number of correctly classified positive classes divide by the total number of positive classes. Or, out of all the positive classes, how much we have predicted correctly. <br>\n",
        "<br>**Recall should be high.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPtR6f0Uia_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "page(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOfrW52MkcVg",
        "colab_type": "text"
      },
      "source": [
        "**Quiz 3**<br>\n",
        "What is the recall for the Cat identification model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48LWPehBknWw",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title What is the recall for cat in the Cat Model?\n",
        "recall =  0.75#@param {type:\"number\"}\n",
        "if recall == .75:\n",
        "  print(\"Correct!\")\n",
        "else:\n",
        "  print(\"Try again\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZlOpCTOr4hL",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title What is the recall for dog in the Cat Model?\n",
        "recall =  0.75#@param {type:\"number\"}\n",
        "if recall == .67:\n",
        "  print(\"Correct!\")\n",
        "else:\n",
        "  print(\"Try again\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wScHTF79lSia",
        "colab_type": "text"
      },
      "source": [
        "# **Precision:**\n",
        "Precision is defined as the ratio of the total number of correctly classified positive classes divided by the total number of predicted positive classes. Or, out of all the predictive positive classes, how much we predicted correctly.<br>\n",
        " \n",
        " **Precision should be high.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-uGjA1RnKPt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "page(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uEdy-85lbzS",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title What is the precision for cats in the Cat Model?\n",
        "precision =  0#@param {type:\"number\"}\n",
        "if precision == .6:\n",
        "  print(\"Correct!\")\n",
        "else:\n",
        "  print(\"Try Again\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NssUYP1xq9ZF",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title What is the precision for dogs in the Cat Model?\n",
        "precision =  0#@param {type:\"number\"}\n",
        "if precision == .8:\n",
        "  print(\"Correct!\")\n",
        "else:\n",
        "  print(\"Try Again\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko9eSC-tmz1C",
        "colab_type": "text"
      },
      "source": [
        "# **F-score or F1-score**:\n",
        "It is difficult to compare two models with different Precision and Recall. So to make them comparable, we use F-Score. It is the Harmonic Mean of Precision and Recall. As compared to Arithmetic Mean, Harmonic Mean punishes the extreme values more. <br>\n",
        "**F-score should be high.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4D7PtDbm0F-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "page(6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTHhHFS7naCT",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title What is the F-score for the cat in the Cat Model?\n",
        "fscore =  0#@param {type:\"number\"}\n",
        "if fscore == 0.67:\n",
        "  print(\"Correct!\")\n",
        "else:\n",
        "  print(\"Try Again\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tulc00C6sBYE",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title What is the F-score for the dog in the Cat Model?\n",
        "fscore =  0#@param {type:\"number\"}\n",
        "if fscore == 0.73:\n",
        "  print(\"Correct!\")\n",
        "else:\n",
        "  print(\"Try Again\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xxl6Bc6oP3f",
        "colab_type": "text"
      },
      "source": [
        "# **Specificity:**\n",
        "Specificity determines the proportion of actual negatives that are correctly identified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfEG7dHIoSLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "page(7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AtRwf5QoWtG",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title What is the Specificity of the Cat Model?\n",
        "specificity = 0 #@param {type:\"number\"}\n",
        "if specificity== 0.67:\n",
        "  print(\"Correct!\")\n",
        "else:\n",
        "  print(\"Try Again\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loUEyLuaqads",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "actual = ['dog','cat', 'dog', 'cat', 'dog', 'dog', 'cat', 'dog', 'dog', 'cat']\n",
        "predicted = ['dog', 'dog', 'dog', 'cat', 'dog', 'dog', 'cat', 'cat', 'cat', 'cat']\n",
        "results = confusion_matrix(actual, predicted)\n",
        "print ('Confusion Matrix :')\n",
        "print(results)\n",
        "print ('Accuracy Score :',accuracy_score(actual, predicted))\n",
        "print('Classification Report : ')\n",
        "print (classification_report(actual, predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU5IBzyZsmrg",
        "colab_type": "text"
      },
      "source": [
        "**Precision** is how certain you are of your true positives. Recall is how certain you are that you are not missing any positives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKgTzNFvsp9K",
        "colab_type": "text"
      },
      "source": [
        "Choose **Recall** if the occurrence of false negatives is unaccepted/intolerable. For example, in the case of diabetes that you would rather have some extra false positives (false alarms) over saving some false negatives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di9LRmauswUr",
        "colab_type": "text"
      },
      "source": [
        "Choose **Precision** if you want to be more confident of your true positives. For example, in case of spam emails, you would rather have some spam emails in your inbox rather than some regular emails in your spam box. You would like to be extra sure that email X is spam before we put it in the spam box."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sURyBLgfs63p",
        "colab_type": "text"
      },
      "source": [
        "Choose **Specificity** if you want to cover all true negatives, i.e. meaning we do not want any false alarms or false positives. For example, in case of a drug test in which all people who test positive will immediately go to jail, you would not want anyone drug-free going to jail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20o-ttjfF2lN",
        "colab_type": "text"
      },
      "source": [
        "# **Why Do We Need a Confusion Matrix?**<br>\n",
        "https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64A3KSy4F4Sh",
        "colab_type": "text"
      },
      "source": [
        "Let’s say you want to predict how many people are infected with a contagious virus in times before they show the symptoms, and isolate them from the healthy population (ringing any bells, yet? 😷). The two values for our target variable would be: Sick and Not Sick.<br>\n",
        "\n",
        "Now, you must be wondering – why do we need a confusion matrix when we have our all-weather friend – Accuracy? Well, let’s see where accuracy falters.<br>\n",
        "\n",
        "Our dataset is an example of an imbalanced dataset. There are 947 data points for the negative class and 3 data points for the positive class. <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AyRaFfPGPsC",
        "colab_type": "text"
      },
      "source": [
        "Let’s see how our model performed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk0xdiGmGeOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "page(9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HFMTQ1sGmoG",
        "colab_type": "text"
      },
      "source": [
        "The total outcome values are:<br>\n",
        "\n",
        "TP = 30, <br>\n",
        "TN = 930, <br>\n",
        "FP = 30, <br>\n",
        "FN = 10<br>\n",
        "\n",
        "So, the accuracy for our model turns out to be:  Accuracy 96% <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcPZ-1lbG54W",
        "colab_type": "text"
      },
      "source": [
        "But it is giving the wrong idea about the result. <br>\n",
        "\n",
        "*Our model is saying “I can predict sick people 96% of the time”. However, it is doing the opposite.* It is predicting the people who will not get sick with 96% accuracy while the sick are spreading the virus!<br>\n",
        "\n",
        "Do you think this is a correct metric for our model given the seriousness of the issue? Shouldn’t we be measuring how many positive cases we can predict correctly to arrest the spread of the contagious virus? Or maybe, out of the correctly predicted cases, how many are positive cases to check the reliability of our model?<br>"
      ]
    }
  ]
}